{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy, os\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# switch for debug\n",
    "flag_print = True\n",
    "\n",
    "# switch to clear existing data\n",
    "flag_clear = True\n",
    "\n",
    "#threshold value for determining section\n",
    "threshold = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skill [u'skill', u'language', u'technology', u'framework', u'tool', u'operate']\n",
      "extra [u'introduction', u'intro', u'achievement', u'hobby', u'link', u'additional', u'personal']\n",
      "exp [u'job', u'internship', u'training', u'research', u'carrer', u'profession', u'project', u'responsibility', u'description']\n",
      "edu [u'education', u'study', u'academic']\n"
     ]
    }
   ],
   "source": [
    "# to get extract sections from the resume -- add or remove from  'similar_to' accordingly\n",
    "similar_to = {\n",
    "    'edu' : ['education', 'study', 'academics'],\n",
    "    'exp' : ['job', 'internship', 'training', 'research', 'carrer', 'profession', \n",
    "             'project', 'responsibility', 'description'],\n",
    "    'skill' : ['skill', 'languages', 'technology', 'frameworks', 'tools', 'operating system'],\n",
    "    'extra' : ['introduction', 'intro', 'achievement', 'hobby', 'links', 'additional', 'personal']\n",
    "}\n",
    "\n",
    "list_of_sections = similar_to.keys()\n",
    "\n",
    "# to bring similar_words to their normal forms\n",
    "for section in list_of_sections:\n",
    "    new_list = []\n",
    "    \n",
    "    for word in similar_to[section]:\n",
    "        docx = nlp(unicode(word))\n",
    "        new_list.append(docx[0].lemma_)\n",
    "        \n",
    "    if flag_print:\n",
    "        print section, new_list\n",
    "        \n",
    "    similar_to[section] = new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello!! --returned--> hello\n",
      ".,<> --returned--> None\n",
      "India --returned--> india\n",
      "of --returned--> None\n",
      "..freedoM.. --returned--> freedom\n",
      "e-mail --returned--> email\n"
     ]
    }
   ],
   "source": [
    "# function to return the words in a uniform \n",
    "def modify(word):\n",
    "    try:\n",
    "        symbols = '''~'`!@#$%^&*)(_+-=}{][|\\:;\",./<>?'''\n",
    "        mod_word = ''\n",
    "        \n",
    "        for char in word:\n",
    "            if (char not in symbols):\n",
    "                mod_word += char.lower()\n",
    "\n",
    "        docx = nlp(unicode(mod_word))\n",
    "\n",
    "        if (len(mod_word) == 0 or docx[0].is_stop):\n",
    "            return None\n",
    "        else:\n",
    "            return docx[0].lemma_\n",
    "    except:\n",
    "        return None # to handle the odd case of characters like 'x02', etc.\n",
    "    \n",
    "if flag_print:\n",
    "    test_words = ['Hello!!', '.,<>', 'India', 'of', '..freedoM..', 'e-mail']\n",
    "    \n",
    "    for word in test_words:\n",
    "        print word, '--returned-->', modify(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if flag_clear:\n",
    "    for file_name in os.listdir(os.getcwd()+'/prc_data'):\n",
    "        os.remove(os.getcwd()+'/prc_data/'+file_name)\n",
    "\n",
    "for file_name in os.listdir(os.getcwd()+'/raw_data'):\n",
    "    if flag_print:\n",
    "        print '\\n'\n",
    "        print '*'*25\n",
    "        print file_name\n",
    "        print '*'*25\n",
    "        \n",
    "    main_file_handler = open('raw_data/'+file_name, 'r')    \n",
    "    previous_section  = 'extra'\n",
    "    \n",
    "    for line in main_file_handler:\n",
    "        # skip line if empty\n",
    "        if (len(line.strip()) == 0):\n",
    "            continue\n",
    "        \n",
    "        if flag_print:\n",
    "            print previous_section, '\\t**', line.strip()\n",
    "        \n",
    "        # processing next line\n",
    "        list_of_words_in_line = line.split()\n",
    "        list_of_imp_words_in_line  = []\n",
    "        \n",
    "        for i in range(len(list_of_words_in_line)):\n",
    "            modified_word = modify(list_of_words_in_line[i])\n",
    "            \n",
    "            if (modified_word):\n",
    "                list_of_imp_words_in_line.append(modified_word)\n",
    "\n",
    "        if (1 <= len(list_of_imp_words_in_line)):\n",
    "            curr_line = ' '.join(list_of_imp_words_in_line)\n",
    "            doc = nlp(unicode(curr_line))\n",
    "            section_value = {}\n",
    "            \n",
    "            # initializing section values to zero\n",
    "            for section in list_of_sections:\n",
    "                section_value[section] = 0.0\n",
    "                \n",
    "            # updating section values    \n",
    "            for token in doc:\n",
    "                for section in list_of_sections:\n",
    "                    for word in similar_to[section]:\n",
    "                        word_token = doc.vocab[unicode(word)]\n",
    "                        section_value[section] = max(section_value[section], float(word_token.similarity(token)))\n",
    "\n",
    "            # determining the next section based on section values and threshold\n",
    "            most_likely_section = 'extra'\n",
    "            for section in list_of_sections:\n",
    "                #print '>>', section, section_value[section]\n",
    "                if (section_value[most_likely_section] < section_value[section] and section_value[section] > threshold):\n",
    "                    most_likely_section = section\n",
    "            \n",
    "            # updating the section\n",
    "            if (previous_section != most_likely_section):\n",
    "                previous_section = most_likely_section\n",
    "\n",
    "        # writing data to the separate files\n",
    "        temp_file_handler = open('prc_data/'+file_name+'_'+previous_section, 'a')\n",
    "        temp_file_handler.write(line)\n",
    "        temp_file_handler.close()\n",
    "        \n",
    "    main_file_handler.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
